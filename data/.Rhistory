rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry =", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning (long run-time)? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry =", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
library(reticulate)
library(reticulate)
use_virtualenv("C:/venvs/r-tensorflow", required = TRUE)
source("~/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code/04_machine_learning_models.R", echo=TRUE)
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
# Load in cleaned data
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/cleaned_temperature_data.csv"
temperature_data <- read.csv(url)
# Ensure Data column is in Date format is numeric
temperature_data$Year <- as.numeric(as.character(temperature_data$Year))
temperature_data$Month <- as.numeric(as.character(temperature_data$Month))
temperature_data$Day <- as.numeric(as.character(temperature_data$Day))
temperature_data$Date <- make_date(temperature_data$Year, temperature_data$Month, temperature_data$Day)
temperature_data$doy <- yday(temperature_data$Date)
# Data Partitioning
training_data <- temperature_data %>% filter(Year <= 2015)
validation_data <- temperature_data %>% filter(Year > 2015 & Year <= 2019)
cat("Training rows:", nrow(training_data), "\n")
cat("Validation rows:", nrow(validation_data), "\n")
# Creating a time series object: Note: ignoring leap years in this analysis
start_year <- training_data$Year[1]
start_doy <- yday(training_data$Date[1])
ts_train <- ts(training_data$Av.temp,
frequency = 365,
start = c(start_year, start_doy))
# ACF and PACF
acf(ts_train, main = "ACF of Training Data (Daily Average Temperature)")
pacf(ts_train, main = "PACF of Training Data (Daily Average Temperature)")
library(reticulate)
use_virtualenv("C:/venvs/r-tensorflow", required = TRUE)
# 4. Machine Learning Models  -------------------------------------
message("4/5 | Running Machine Learning Models...")
source("04_machine_learning_models.R")
setwd("C:/Users/Harvey Collins/OneDrive/Documents/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code")
# 4. Machine Learning Models  -------------------------------------
message("4/5 | Running Machine Learning Models...")
source("04_machine_learning_models.R")
q
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
library(randomForest)
library(caret)
library(tensorflow)
library(keras)
library(ggplot2)
library(reticulate)
#use_virtualenv("C:/venvs/r-tensorflow", required = TRUE
# Set seed
set.seed(123)
tensorflow::set_random_seed(123)
# Load in cleaned data
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/cleaned_temperature_data.csv"
temperature_data <- read.csv(url)
# Ensure Data column is in Date format is numeric
temperature_data$Year <- as.numeric(as.character(temperature_data$Year))
temperature_data$Month <- as.numeric(as.character(temperature_data$Month))
temperature_data$Day <- as.numeric(as.character(temperature_data$Day))
temperature_data$Date <- make_date(temperature_data$Year, temperature_data$Month, temperature_data$Day)
temperature_data$doy <- yday(temperature_data$Date)
temperature_data$time_index <- as.numeric(temperature_data$Date)
# Data Partitioning
training_data <- temperature_data %>% filter(Year <= 2015)
validation_data <- temperature_data %>% filter(Year > 2015 & Year <= 2019)
cat("Training rows:", nrow(training_data), "\n")
cat("Validation rows:", nrow(validation_data), "\n")
# Detrending and Deseasoning
training_data$Date_numeric <- as.numeric(training_data$Date)
fit_cubic <- lm(Av.temp ~ poly(Date_numeric, 3), data = training_data)
training_data$trend <- predict(fit_cubic, newdata = training_data)
training_data$resid <- training_data$Av.temp - training_data$trend
daily_median <- training_data %>%
group_by(doy) %>%
summarise(median_resid = median(resid, na.rm = TRUE))
training_data <- training_data %>% left_join(daily_median, by = "doy")
training_data$best_resid <- training_data$resid - training_data$median_resid
validation_data$Date_numeric <- as.numeric(validation_data$Date)
validation_data$trend <- predict(fit_cubic, newdata = validation_data)
validation_data$resid <- validation_data$Av.temp - validation_data$trend
validation_data <- validation_data %>% left_join(daily_median, by = "doy")
validation_data$best_resid <- validation_data$resid - validation_data$median_resid
ggplot(training_data, aes(x = Date, y = best_resid)) +
geom_line(color = "purple") +
labs(title = "Deseasoned Residuals (Training Data)", x = "Date", y = "Residuals (C)") +
theme_minimal()
# Predictor features and target
features <- c("Year", "Month", "Day", "doy", "time_index")
target <- "Av.temp"
# Prepare training for validation datasets for ML:
x_train <- training_data %>% select(all_of(features))
y_train <- training_data$best_resid
x_val <- validation_data %>% select(all_of(features))
y_val <- validation_data$best_resid
# Scaling the data
scale_params <- preProcess(x_train, method = c("center", "scale"))
x_train_scaled <- predict(scale_params, x_train)
x_val_scaled <- predict(scale_params, x_val)
# Random Forest Model
rf_model <- randomForest(x = x_train_scaled, y = y_train, ntree = 500)
rf_predictions <- predict(rf_model, newdata = x_val_scaled)
rf_rmse <- rmse(y_val, rf_predictions)
rf_mae <- mae(y_val, rf_predictions)
cat("Baseline Random Forest RMSE:", rf_rmse, "\n")
cat("Baseline Random Forest MAE:", rf_mae, "\n")
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning (long run-time)? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Tuned hyperparameters: mtry =", best_mtry,
" ntree=", optimal_ntree, "\n\n")
final_rf_model <- randomForest(
x = x_train_scaled, y = y_train,
mtry = best_mtry,
ntree = optimal_ntree,
nodesize = 10
)
final_rf_predictions <- predict(final_rf_model, x_val_scaled)
final_rf_rmse <- rmse(y_val, final_rf_predictions)
final_rf_mae <- mae(y_val, final_rf_predictions)
} else {
cat("Skipping tuning, reusing baseline model\n\n")
final_rf_model <- rf_model
final_rf_predictions <- rf_predictions
final_rmse <- rf_rmse
final_mae <- rf_mae
}
}
# 4. Machine Learning Models  -------------------------------------
message("4/5 | Running Machine Learning Models...")
source("04_machine_learning_models.R")
# 5. 2020 Forecasts -----------------------------------------------
message("5/5 | Generating 2020 Forecasts...")
source("05_2020_forecasts.R")
cat("Deep Keras RMSE:", rmse_deep, "\n")
cat("Deep Keras MAE:", mae_deep, "\n")
cat("Deep Keras Combined", rmse_deep + mae_deep "\n")
# 5. 2020 Forecasts -----------------------------------------------
message("5/5 | Generating 2020 Forecasts...")
source("05_2020_forecasts.R")
setwd("C:/Users/Harvey Collins/OneDrive/Documents/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code")
# 5. 2020 Forecasts -----------------------------------------------
message("5/5 | Generating 2020 Forecasts...")
source("05_2020_forecasts.R")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Removing rows that have values as NA
temperature_data <- temperature_data[!apply(is.na(temperature_data), 1, all), ]
temperature_data <- na.omit(temperature_data)
# Convert the date column into a date type
temperature_data$Date <- dmy(temperature_data$Date)
str(temperature_data$Date)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
bad_rows <- temperature_data %>%
filter(!consistent)
write.csv(bad_rows, "inconsistent.csv", row.names = FALSE)
Av.temp >= Tmin & Av.temp <= Tmax
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
# Identify potential outliers
summary_stats <- summary(temperature_data$Av.temp)
print("Summary statistics for daily average temperature:")
print(summary_stats)
# Convert the date column into a date type
temperature_data$Date <- dmy(temperature_data$Date)
str(temperature_data$Date)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Removing rows that have values as NA
temperature_data <- temperature_data[!apply(is.na(temperature_data), 1, all), ]
temperature_data <- na.omit(temperature_data)
# Convert the date column into a date type
temperature_data$Date <- dmy(temperature_data$Date)
str(temperature_data$Date)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Removing rows that have values as NA
#temperature_data <- temperature_data[!apply(is.na(temperature_data), 1, all), ]
#temperature_data <- na.omit(temperature_data)
temperature_data <- temperature_data %>%
filter(
!is.na(Av.temp),
!is.na(Tmin),
!is.na(Tmax)
)
# Convert the date column into a date type
temperature_data$Date <- dmy(temperature_data$Date)
str(temperature_data$Date)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Removing rows that have values as NA
#temperature_data <- temperature_data[!apply(is.na(temperature_data), 1, all), ]
#temperature_data <- na.omit(temperature_data)
temperature_data <- temperature_data[
complete.cases( temperature_data[, c("Av.temp", "Tmin", "Tmax")] ),
]
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Removing rows that have values as NA
na_per_col <- sapply(temperature_data, function(x) sum(is.na(x)))
print(na_per_col)
rows_with_na <- which(!complete.cases(temperature_data))
cat("Rows containing at least one NA:\n", rows_with_na, "\n\n")
print( temperature_data[ rows_with_na, ])
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
# Removing rows that have values as NA
na_per_col <- sapply(temperature_data, function(x) sum(is.na(x)))
print(na_per_col)
rows_with_na <- which(!complete.cases(temperature_data))
cat("Rows containing at least one NA:\n", rows_with_na, "\n\n")
print( temperature_data[ rows_with_na, ])
temperature_data <- temperature_data[-rows_with_na, ]
sum(!complete.cases(temperature_data))
source("~/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code/01_data_preparation.R", echo=TRUE)
source("~/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code/02_exploratory_analysis.R", echo=TRUE)
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
# Load in cleaned data
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/cleaned_temperature_data.csv"
temperature_data <- read.csv(url)
# Ensure Data column is in Date format is numeric
temperature_data$Year <- as.numeric(as.character(temperature_data$Year))
temperature_data$Month <- as.numeric(as.character(temperature_data$Month))
temperature_data$Day <- as.numeric(as.character(temperature_data$Day))
temperature_data$Date <- make_date(temperature_data$Year, temperature_data$Month, temperature_data$Day)
temperature_data$doy <- yday(temperature_data$Date)
# Data Partitioning
training_data <- temperature_data %>% filter(Year <= 2015)
validation_data <- temperature_data %>% filter(Year > 2015 & Year <= 2019)
cat("Training rows:", nrow(training_data), "\n")
cat("Validation rows:", nrow(validation_data), "\n")
# Creating a time series object: Note: ignoring leap years in this analysis
start_year <- training_data$Year[1]
start_doy <- yday(training_data$Date[1])
ts_train <- ts(training_data$Av.temp,
frequency = 365,
start = c(start_year, start_doy))
# ACF and PACF
acf(ts_train, main = "ACF of Training Data (Daily Average Temperature)")
pacf(ts_train, main = "PACF of Training Data (Daily Average Temperature)")
# Deseasoning via Regression
training_data$Date_numeric <- as.numeric(training_data$Date)
validation_data$Date_numeric <- as.numeric(validation_data$Date)
# Fit cubic regression to capture trend
fit_cubic <- lm(Av.temp ~ poly(Date_numeric, 3), data = training_data)
training_data$trend <- predict(fit_cubic, newdata = training_data)
# Calc residuals (detrended values)
training_data$resid <- training_data$Av.temp - training_data$trend
# Compute seasonal component from training data
daily_median <- training_data %>%
group_by(doy) %>%
summarise(median_resid = median(resid, na.rm = TRUE))
training_data <- training_data %>% left_join(daily_median, by = "doy")
training_data$best_resid <- training_data$resid - training_data$median_resid
# Plot deseasoned residuals
ggplot(training_data, aes(x = Date, y = best_resid)) +
geom_line(color = "purple") +
labs(title = "Deseasoned Residuals (Training Data)", x = "Date", y = "Residual (C)") +
theme_minimal()
ts_best_resid <- ts(training_data$best_resid, frequency = 365,
start = c(min(training_data$Year), yday(min(training_data$Date))))
# Modelling the Deseasoned Residuals
h_val <- nrow(validation_data)
# Fit ARIMA model
fit_arima_deseason <- auto.arima(ts_best_resid, seasonal = FALSE)
forecast_best_resid_arima <- forecast(fit_arima_deseason, h = h_val)
# Fit ETS model
fit_ets_deseason <- ets(as.numeric(training_data$best_resid))
forecast_best_resid_ets <- forecast(fit_ets_deseason, h = h_val)
# TBATS on Deseasoned Residuals
fit_tbats_deseason <- tbats(ts_best_resid)
forecast_best_resid_tbats <- forecast(fit_tbats_deseason, h = h_val)
# Reconstruct forecasted temperature for validation period
validation_data$trend <- predict(fit_cubic, newdata = validation_data)
validation_data <- validation_data %>% left_join(daily_median, by = "doy")
# ARIMA-based forecast
forecasted_resid_arima <- as.numeric(forecast_best_resid_arima$mean)
validation_data$pred_temp_arima <- validation_data$trend + validation_data$median_resid + forecasted_resid_arima
# ETS-based forecast
forecasted_resid_ets <- as.numeric(forecast_best_resid_ets$mean)
validation_data$pred_temp_ets <- validation_data$trend + validation_data$median_resid + forecasted_resid_ets
# TBATS-based forecast
forecasted_resid_tbats <- as.numeric(forecast_best_resid_tbats$mean)
validation_data$pred_temp_tbats <- validation_data$trend + validation_data$median_resid + forecasted_resid_tbats
# Compute error metrics
rmse_arima_deseason <- rmse(validation_data$Av.temp, validation_data$pred_temp_arima)
mae_arima_deseason <- mae(validation_data$Av.temp, validation_data$pred_temp_arima)
rmse_ets_deseason <- rmse(validation_data$Av.temp, validation_data$pred_temp_ets)
mae_ets_deseason <- mae(validation_data$Av.temp, validation_data$pred_temp_ets)
rmse_tbats_deseason <- rmse(validation_data$Av.temp, validation_data$pred_temp_tbats)
mae_tbats_deseason <- mae(validation_data$Av.temp, validation_data$pred_temp_tbats)
cat("ARIMA RMSE:", rmse_arima_deseason, "\n")
cat("ARIMA MAE:", mae_arima_deseason, "\n")
cat("ETS RMSE:", rmse_ets_deseason, "\n")
cat("ETS MAE:", mae_ets_deseason, "\n")
cat("TBATS RMSE:", rmse_tbats_deseason, "\n")
cat("TBATS MAE:", mae_tbats_deseason, "\n")
