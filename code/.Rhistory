# Compute the estimated trend and residuals for the training data
training_data <- training_data %>%
mutate(trend = predict(fit_trend, newdata = training_data),
resid = Av.temp - trend)
# Apply the same transformation to the validation data
validation_data <- validation_data %>%
mutate(trend = predict(fit_trend, newdata = validation_data),
resid = Av.temp - trend)
# Predictor features and target
features <- c("Year", "Month", "Day", "doy", "time_index")
target <- "Av.temp"
# Prepare training for validation datasets for ML:
x_train <- training_data %>% select(all_of(features))
y_train <- training_data$Av.temp
x_val <- validation_data %>% select(all_of(features))
y_val <- validation_data$Av.temp
# Scaling the data
scale_params <- preProcess(x_train, method = c("center", "scale"))
x_train_scaled <- predict(scale_params, x_train)
x_val_scaled <- predict(scale_params, x_val)
# Preparation for Deep Learning Model (Keras)
tensorflow::set_random_seed(123)
# Convert data to matrices and numpy to arrays
train_x <- as.matrix(x_train_scaled)
validation_x <- as.matrix(x_val_scaled)
train_y <- matrix(as.numeric(y_train), ncol = 1)
validation_y <- matrix(as.numeric(y_val), ncol = 1)
train_y <- matrix(train_y, ncol = 1)
validation_y <- matrix(validation_y, ncol = 1)
cat("Dimensions of train_x:", dim(train_x), "\n")
cat("Dimensions of train_y:", dim(train_y), "\n")
#np <- import("numpy")
#train_x <- np$array(train_x, dtype = "float32")
#validation_x <- np$array(validation_x, dtype = "float32")
#train_y <- np$array(train_y, dtype = "float32")
#validation_y <- np$array(validation_y, dtype = "float32")
# Deep Neural Network Model using Keras
l2_reg <- tf$keras$regularizers$l2(1e-5)
input <- layer_input(shape = c(as.integer(ncol(train_x))))
output <- input %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = l2_reg) %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 32, activation = "relu", kernel_regularizer = l2_reg) %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 1, activation = "linear")
model <- keras_model(inputs = input, outputs = output)
# Compile the model
model$compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mae")
)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 5)
history <- model$fit(
x = train_x,
y = train_y,
epochs = as.integer(75),
batch_size = as.integer(100),
validation_data = list(validation_x, validation_y),
callbacks = list(early_stop),
verbose = 1
)
# Predict on validation data
dnn_predictions <- model$predict(validation_x)
dnn_rmse <- rmse(validation_y, dnn_predictions)
dnn_mae <- mae(validation_y, dnn_predictions)
cat("Deep Neural Network RMSE:", dnn_rmse, "\n")
cat("Deep Neural Network MAE:", dnn_mae, "\n")
# Compare Models
results <- data.frame(
Model = c("Random Forest", "Deep Neural Network"),
RMSE = c(rf_rmse, dnn_rmse),
MAE = c(rf_mae, dnn_mae)
)
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
library(randomForest)
library(caret)
library(tensorflow)
library(keras)
# Set seed
set.seed(123)
tensorflow::set_random_seed(123)
# Load in cleaned data
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/cleaned_temperature_data.csv"
temperature_data <- read.csv(url)
# Ensure Data column is in Date format is numeric
temperature_data$Year <- as.numeric(as.character(temperature_data$Year))
temperature_data$Month <- as.numeric(as.character(temperature_data$Month))
temperature_data$Day <- as.numeric(as.character(temperature_data$Day))
temperature_data$Date <- make_date(temperature_data$Year, temperature_data$Month, temperature_data$Day)
temperature_data$doy <- yday(temperature_data$Date)
temperature_data$time_index <- as.numeric(temperature_data$Date)
# Data Partitioning
training_data <- temperature_data %>% filter(Year <= 2015)
validation_data <- temperature_data %>% filter(Year > 2015 & Year <= 2019)
cat("Training rows:", nrow(training_data), "\n")
cat("Validation rows:", nrow(validation_data), "\n")
# Detrending Step: Fit a cubic regression on the time_index to capture the trend
#fit_trend <- lm(Av.temp ~ poly(time_index, 3), data = training_data)
# Compute the estimated trend and residuals for the training data
#training_data <- training_data %>%
#  mutate(trend = predict(fit_trend, newdata = training_data),
#         resid = Av.temp - trend)
# Apply the same transformation to the validation data
#validation_data <- validation_data %>%
#  mutate(trend = predict(fit_trend, newdata = validation_data),
#         resid = Av.temp - trend)
# Predictor features and target
features <- c("Year", "Month", "Day", "doy", "time_index")
target <- "Av.temp"
# Prepare training for validation datasets for ML:
x_train <- training_data %>% select(all_of(features))
y_train <- training_data$Av.temp
x_val <- validation_data %>% select(all_of(features))
y_val <- validation_data$Av.temp
# Scaling the data
scale_params <- preProcess(x_train, method = c("center", "scale"))
x_train_scaled <- predict(scale_params, x_train)
x_val_scaled <- predict(scale_params, x_val)
# Preparation for Deep Learning Model (Keras)
tensorflow::set_random_seed(123)
# Convert data to matrices and numpy to arrays
train_x <- as.matrix(x_train_scaled)
validation_x <- as.matrix(x_val_scaled)
train_y <- matrix(as.numeric(y_train), ncol = 1)
validation_y <- matrix(as.numeric(y_val), ncol = 1)
train_y <- matrix(train_y, ncol = 1)
validation_y <- matrix(validation_y, ncol = 1)
cat("Dimensions of train_x:", dim(train_x), "\n")
cat("Dimensions of train_y:", dim(train_y), "\n")
#np <- import("numpy")
#train_x <- np$array(train_x, dtype = "float32")
#validation_x <- np$array(validation_x, dtype = "float32")
#train_y <- np$array(train_y, dtype = "float32")
#validation_y <- np$array(validation_y, dtype = "float32")
# Deep Neural Network Model using Keras
l2_reg <- tf$keras$regularizers$l2(1e-5)
input <- layer_input(shape = c(as.integer(ncol(train_x))))
output <- input %>%
layer_dense(units = 64, activation = "relu", kernel_regularizer = l2_reg) %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 32, activation = "relu", kernel_regularizer = l2_reg) %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 1, activation = "linear")
model <- keras_model(inputs = input, outputs = output)
# Compile the model
model$compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mae")
)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 5)
history <- model$fit(
x = train_x,
y = train_y,
epochs = as.integer(75),
batch_size = as.integer(100),
validation_data = list(validation_x, validation_y),
callbacks = list(early_stop),
verbose = 1
)
# Predict on validation data
dnn_predictions <- model$predict(validation_x)
dnn_rmse <- rmse(validation_y, dnn_predictions)
dnn_mae <- mae(validation_y, dnn_predictions)
cat("Deep Neural Network RMSE:", dnn_rmse, "\n")
cat("Deep Neural Network MAE:", dnn_mae, "\n")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
library(randomForest)
library(caret)
library(tensorflow)
library(keras)
library(ggplot2)
library(reticulate)
#use_virtualenv("C:/venvs/r-tensorflow", required = TRUE
# Set seed
set.seed(123)
# Load in cleaned data
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/cleaned_temperature_data.csv"
temperature_data <- read.csv(url)
# Ensure Data column is in Date format is numeric
temperature_data$Year <- as.numeric(as.character(temperature_data$Year))
temperature_data$Month <- as.numeric(as.character(temperature_data$Month))
temperature_data$Day <- as.numeric(as.character(temperature_data$Day))
temperature_data$Date <- make_date(temperature_data$Year, temperature_data$Month, temperature_data$Day)
temperature_data$doy <- yday(temperature_data$Date)
temperature_data$time_index <- as.numeric(temperature_data$Date)
# Data Partitioning
training_data <- temperature_data %>% filter(Year <= 2015)
validation_data <- temperature_data %>% filter(Year > 2015 & Year <= 2019)
cat("Training rows:", nrow(training_data), "\n")
cat("Validation rows:", nrow(validation_data), "\n")
# Detrending and Deseasoning
training_data$Date_numeric <- as.numeric(training_data$Date)
fit_cubic <- lm(Av.temp ~ poly(Date_numeric, 3), data = training_data)
training_data$trend <- predict(fit_cubic, newdata = training_data)
training_data$resid <- training_data$Av.temp - training_data$trend
daily_median <- training_data %>%
group_by(doy) %>%
summarise(median_resid = median(resid, na.rm = TRUE))
training_data <- training_data %>% left_join(daily_median, by = "doy")
training_data$best_resid <- training_data$resid - training_data$median_resid
validation_data$Date_numeric <- as.numeric(validation_data$Date)
validation_data$trend <- predict(fit_cubic, newdata = validation_data)
validation_data$resid <- validation_data$Av.temp - validation_data$trend
validation_data <- validation_data %>% left_join(daily_median, by = "doy")
validation_data$best_resid <- validation_data$resid - validation_data$median_resid
ggplot(training_data, aes(x = Date, y = best_resid)) +
geom_line(color = "purple") +
labs(title = "Deseasoned Residuals (Training Data)", x = "Date", y = "Residuals (C)") +
theme_minimal()
# Predictor features and target
features <- c("Year", "Month", "Day", "doy", "time_index")
target <- "Av.temp"
# Prepare training for validation datasets for ML:
x_train <- training_data %>% select(all_of(features))
y_train <- training_data$best_resid
x_val <- validation_data %>% select(all_of(features))
y_val <- validation_data$best_resid
# Scaling the data
scale_params <- preProcess(x_train, method = c("center", "scale"))
x_train_scaled <- predict(scale_params, x_train)
x_val_scaled <- predict(scale_params, x_val)
# Random Forest Model
rf_model <- randomForest(x = x_train_scaled, y = y_train, ntree = 500)
rf_predictions <- predict(rf_model, newdata = x_val_scaled)
rf_rmse <- rmse(y_val, rf_predictions)
rf_mae <- mae(y_val, rf_predictions)
cat("Baseline Random Forest RMSE:", rf_rmse, "\n")
cat("Baseline Random Forest MAE:", rf_mae, "\n")
cat("\nDo you want to run hyper-parameter tuning (long run-time)?\n")
choice <- menu(c("Yes", "No"), title = NULL, graphics = FALSE)
cat("\nDo you want to run hyper-parameter tuning (long run-time)?\n")
choice <- menu(c("Yes", "No"), title = NULL, graphics = FALSE)
cat("\nDo you want to run hyper-parameter tuning (long run-time)?\n")
choice <- menu(c("Yes", "No"), title = NULL, graphics = FALSE)
repeat {
ans <- tolower(readline("Run hyperparameter tuning? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry=", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry =", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning (long run-time)? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry =", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning (long run-time)? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry =", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
{
repeat {
ans <- tolower(readline("Run hyperparameter tuning (long run-time)? (y/n): "))
if (ans %in% c("y", "n")) break
cat("Please enter 'y' or 'n'\n")
}
ntree_values <- seq(200, 500, by = 100)
if (ans == "y") {
# Tuning mtry with caret
rf_grid <- expand.grid(mtry = 1:ncol(x_train_scaled))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- caret::train(x = x_train_scaled,
y = y_train,
method = "rf",
tuneGrid = rf_grid,
trControl = control,
ntree = 400)
print(rf_tuned)
plot(rf_tuned)
best_mtry <- rf_tuned$bestTune$mtry
cat("Best mtry from tuning:", best_mtry, "\n\n")
ntree_errors <- sapply(ntree_values, function(ntree) {
model <- randomForest(x = x_train_scaled, y = y_train, ntree = ntree, mtry = best_mtry)
preds <- predict(model, newdata = x_val_scaled)
rmse(y_val, preds)
})
plot(ntree_values, ntree_errors, type = "b",
xlab = "Number of Trees", ylab = "Validation RMSE",
main = "Tuning ntree for Random Forest")
optimal_ntree <- ntree_values[which.min(ntree_errors)]
cat("Optimal ntree:", optimal_ntree, "\n")
} else {
cat("Skipping tuning, using default mtry")
best_mtry <- floor(sqrt(ncol(x_train_scaled)))
optimal_ntree <- 500
cat("Using mtry =", best_mtry, "and ntree =", optimal_ntree, "\n\n")
}
}
library(reticulate)
library(reticulate)
use_virtualenv("C:/venvs/r-tensorflow", required = TRUE)
source("~/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code/04_machine_learning_models.R", echo=TRUE)
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
# Load in cleaned data
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/cleaned_temperature_data.csv"
temperature_data <- read.csv(url)
# Ensure Data column is in Date format is numeric
temperature_data$Year <- as.numeric(as.character(temperature_data$Year))
temperature_data$Month <- as.numeric(as.character(temperature_data$Month))
temperature_data$Day <- as.numeric(as.character(temperature_data$Day))
temperature_data$Date <- make_date(temperature_data$Year, temperature_data$Month, temperature_data$Day)
temperature_data$doy <- yday(temperature_data$Date)
# Data Partitioning
training_data <- temperature_data %>% filter(Year <= 2015)
validation_data <- temperature_data %>% filter(Year > 2015 & Year <= 2019)
cat("Training rows:", nrow(training_data), "\n")
cat("Validation rows:", nrow(validation_data), "\n")
# Creating a time series object: Note: ignoring leap years in this analysis
start_year <- training_data$Year[1]
start_doy <- yday(training_data$Date[1])
ts_train <- ts(training_data$Av.temp,
frequency = 365,
start = c(start_year, start_doy))
# ACF and PACF
acf(ts_train, main = "ACF of Training Data (Daily Average Temperature)")
pacf(ts_train, main = "PACF of Training Data (Daily Average Temperature)")
library(dplyr)
library(ggplot2)
library(lubridate)
library(forecast)
library(Metrics)
library(randomForest)
library(caret)
library(tensorflow)
library(keras)
library(readxl)
library(patchwork)
setwd("C:/Users/Harvey Collins/OneDrive/Documents/MISCADA/Earth & Environmental Sciences/Mini_Project/temperature-prediction/code")
# 1. Data Preparation ---------------------------------------------
message("1/5 | Running Data Preparation...")
source("01_data_preparation.R")
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
# Load in the daily temperature
url <- "https://raw.githubusercontent.com/harveywcollins/temperature-prediction/refs/heads/main/data/durhamtemp_1901_2019.csv"
temperature_data <- read.csv(url)
head(temperature_data)
str(temperature_data)
summary(temperature_data)
# Verify that temperature values are consistent
temperature_data <- temperature_data %>%
mutate(consistent = ifelse(Av.temp >= Tmin & Av.temp <= Tmax, TRUE, FALSE))
inconsistencies <- sum(!temperature_data$consistent)
cat("Number of inconsistent rows (Av.temp not between Tmin and Tmax):",  inconsistencies, "\n")
